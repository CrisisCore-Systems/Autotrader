# Enhanced LLM Configuration with Cost Controls, Fallbacks, and Audit Settings
# Version: 2.0.0

llm:
  # Provider configurations with cost and performance characteristics
  providers:
    gpt-small:
      model: gpt-4o-mini
      provider: openai
      cost_per_1k_input: 0.00015
      cost_per_1k_output: 0.0006
      max_tokens: 4096
      context_window: 128000
      supports_json_mode: true
      
    gpt-mid:
      model: gpt-4o
      provider: openai
      cost_per_1k_input: 0.0025
      cost_per_1k_output: 0.01
      max_tokens: 16384
      context_window: 128000
      supports_json_mode: true
      
    gpt-premium:
      model: gpt-4-turbo
      provider: openai
      cost_per_1k_input: 0.01
      cost_per_1k_output: 0.03
      max_tokens: 4096
      context_window: 128000
      supports_json_mode: true
      
    claude-fast:
      model: claude-3-haiku-20240307
      provider: anthropic
      cost_per_1k_input: 0.00025
      cost_per_1k_output: 0.00125
      max_tokens: 4096
      context_window: 200000
      supports_json_mode: false
      
    claude-smart:
      model: claude-3-5-sonnet-20241022
      provider: anthropic
      cost_per_1k_input: 0.003
      cost_per_1k_output: 0.015
      max_tokens: 8192
      context_window: 200000
      supports_json_mode: true
      
    groq-fast:
      model: llama-3.1-70b-versatile
      provider: groq
      cost_per_1k_input: 0.00059
      cost_per_1k_output: 0.00079
      max_tokens: 8192
      context_window: 128000
      supports_json_mode: true
      inference_speed: ultra  # groq's hardware acceleration

  # Route-specific model assignments with fallback chains
  routes:
    narrative_summary:
      primary: gpt-mid
      fallback: [claude-smart, gpt-small]
      max_retries: 2
      timeout_seconds: 30
      
    contract_explainer:
      primary: claude-smart
      fallback: [gpt-mid, claude-fast]
      max_retries: 3
      timeout_seconds: 45
      require_json_mode: true
      
    sentiment_bucket:
      primary: gpt-small
      fallback: [groq-fast, claude-fast]
      max_retries: 2
      timeout_seconds: 15
      
    rare_deep_report:
      primary: gpt-premium
      fallback: [claude-smart, gpt-mid]
      max_retries: 1
      timeout_seconds: 120
      
    technical_pattern:
      primary: groq-fast
      fallback: [gpt-small, claude-fast]
      max_retries: 2
      timeout_seconds: 20
      
    onchain_activity:
      primary: gpt-mid
      fallback: [claude-smart]
      max_retries: 2
      timeout_seconds: 30

  # Budget controls and cost management
  budget:
    daily_usd_cap: 15.00
    per_job_usd_cap: 0.75
    per_route_hourly_cap:
      narrative_summary: 5.00
      contract_explainer: 8.00
      sentiment_bucket: 2.00
      rare_deep_report: 10.00
    alert_at_percent: 80  # Alert when reaching 80% of budget
    pause_at_percent: 95  # Pause new requests at 95% of budget
    reset_hour_utc: 0     # Reset daily budget at midnight UTC

  # Concurrency and rate limiting
  concurrency:
    max_concurrent_requests: 10
    per_provider_max:
      openai: 5
      anthropic: 5
      groq: 10  # Higher limit for groq's speed
    per_route_max:
      narrative_summary: 3
      contract_explainer: 2
      sentiment_bucket: 5
      rare_deep_report: 1

  # Rate limiting (requests per time window)
  rate_limits:
    openai:
      requests_per_minute: 500
      tokens_per_minute: 150000
    anthropic:
      requests_per_minute: 50
      tokens_per_minute: 100000
    groq:
      requests_per_minute: 30
      tokens_per_minute: 14400  # groq's rate limit

  # Retry and backoff policies
  retry_policies:
    exponential_backoff:
      initial_delay_ms: 1000
      max_delay_ms: 60000
      multiplier: 2.0
      jitter: true
      
    retry_on_status_codes: [429, 500, 502, 503, 504]
    retry_on_errors:
      - "rate_limit_exceeded"
      - "server_error"
      - "timeout"
      - "connection_error"
    
    max_total_timeout_seconds: 300  # 5 minutes max per request chain

  # Caching configuration
  caching:
    enabled: true
    backend: redis  # or 'memory' for development
    semantic_ttl_hours: 12
    exact_ttl_hours: 24
    max_cache_size_mb: 512
    eviction_policy: lru
    
    # Cache keys based on
    key_components:
      - prompt_hash
      - model_id
      - temperature
      - max_tokens
    
    # Skip caching for these routes
    skip_routes:
      - rare_deep_report  # Always fresh

  # Prompt template validation
  validation:
    enforce_json_schema: true
    schema_version_check: true
    reject_extra_keys: true
    max_prompt_tokens: 32000
    max_output_tokens: 8192

  # Audit and logging settings
  audit:
    log_all_requests: true
    log_level: info
    log_prompts: true
    log_completions: true
    
    # PII redaction patterns (regex)
    redact_patterns:
      - '\b0x[a-fA-F0-9]{40}\b'  # Ethereum addresses (optional)
      - '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'  # Emails
      - '\b\d{3}-\d{2}-\d{4}\b'  # SSN-like patterns
    
    # Store prompts/completions for analysis
    store_samples:
      enabled: true
      sample_rate: 0.1  # 10% sampling
      retention_days: 30
      storage_path: data/llm_audit
    
    # Cost tracking
    track_costs: true
    cost_alert_threshold_usd: 10.0
    export_cost_reports: true
    cost_report_schedule: "0 1 * * *"  # Daily at 1 AM

  # Error handling policies
  error_handling:
    # Return partial results on timeout
    allow_partial_results: false
    
    # Fallback to cached result on error
    use_cache_on_error: true
    cache_max_age_hours: 48
    
    # Circuit breaker per provider
    circuit_breaker:
      enabled: true
      failure_threshold: 5
      timeout_seconds: 60
      half_open_after_seconds: 30

  # Model-specific parameters
  model_parameters:
    default:
      temperature: 0.3
      top_p: 0.9
      frequency_penalty: 0.0
      presence_penalty: 0.0
      
    by_route:
      narrative_summary:
        temperature: 0.5  # More creative
        top_p: 0.95
        
      contract_explainer:
        temperature: 0.1  # Very deterministic
        top_p: 0.9
        
      sentiment_bucket:
        temperature: 0.2
        top_p: 0.9
        
      rare_deep_report:
        temperature: 0.7  # Most creative
        top_p: 0.95

  # Feature flags
  features:
    enable_streaming: false
    enable_function_calling: true
    enable_vision: false
    enable_batch_processing: true
    
  # Monitoring and observability
  monitoring:
    prometheus_metrics: true
    trace_requests: true
    trace_sampling_rate: 1.0
    
    metrics:
      - llm_request_duration_seconds
      - llm_request_total
      - llm_request_errors_total
      - llm_tokens_used_total
      - llm_cost_usd_total
      - llm_cache_hit_rate
      - llm_fallback_triggered_total

# Security settings
security:
  # API key rotation
  api_key_rotation:
    enabled: false
    rotation_days: 90
    
  # Request signing (optional)
  sign_requests: false
  
  # Rate limit bypass token (for internal services)
  internal_service_token_env: LLM_INTERNAL_TOKEN

# Development/Testing overrides
development:
  mock_llm_responses: false
  use_local_models: false
  local_model_endpoint: http://localhost:8080
  disable_cost_limits: false
  verbose_logging: true
