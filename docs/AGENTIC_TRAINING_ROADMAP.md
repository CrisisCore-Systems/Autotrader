# Agentic System Training & Enhancement Roadmap

**Current Date**: October 18, 2025  
**System Status**: Production-ready (75% WR, 6.0x PF on 6-year backtest)

## ðŸŽ¯ **Training Capabilities Matrix**

| Component | Current State | Training Potential | Priority |
|-----------|---------------|-------------------|----------|
| **Threshold Adaptation** | âœ… Auto-adjusts after 20 trades | Regime-specific learning | HIGH |
| **Agent Weighting** | âŒ Equal votes | Learn which agents are most accurate | HIGH |
| **Sector Learning** | âŒ Generic scoring | Sector-specific gem score offsets | MEDIUM |
| **Timing Optimization** | âŒ Fixed rules | Learn best entry/exit times by pattern | MEDIUM |
| **Regime Detection** | âœ… VIX/SPY rules | ML-based regime classification | LOW |
| **News Sentiment** | âœ… Keyword scoring | Fine-tune sentiment weights | MEDIUM |

---

## ðŸ“Š **Phase 1: Enhanced Learning (Next 30 Days)**

### 1. Agent Performance Weighting
**Current**: Each agent has equal veto power  
**Enhancement**: Weight agents by historical accuracy

```python
class WeightedConsensus:
    """Agent votes weighted by historical accuracy."""
    
    def __init__(self, memory: AgenticMemory):
        self.agent_weights = {
            "forecaster": 1.0,
            "riskofficer": 1.0,
            "newssentry": 1.0,
            "sentinel": 1.0,
        }
    
    def update_weights_from_history(self):
        """Adjust weights based on agent accuracy."""
        # Query agent_performance table
        # If Forecaster has 80% accuracy, increase weight to 1.2
        # If RiskOfficer has 60% accuracy, decrease to 0.8
        pass
    
    def calculate_weighted_vote(self, agent_votes: Dict[str, bool]) -> float:
        """Return 0.0-1.0 confidence score."""
        weighted_sum = sum(
            self.agent_weights[agent] * (1.0 if vote else 0.0)
            for agent, vote in agent_votes.items()
        )
        total_weight = sum(self.agent_weights.values())
        return weighted_sum / total_weight
```

**Implementation Steps**:
1. Track agent prediction accuracy after each trade
2. Update `agent_performance` table with correct_approvals/correct_vetoes
3. Recalculate weights every 10 trades
4. Replace binary veto with weighted consensus score (e.g., need 0.7+ to trade)

**Expected Impact**: +5-10% win rate improvement, fewer false vetoes

---

### 2. Regime-Specific Threshold Learning
**Current**: Single threshold adapts globally  
**Enhancement**: Separate thresholds for each regime

```python
class RegimeAdaptiveLearning:
    """Regime-specific threshold optimization."""
    
    def __init__(self, memory: AgenticMemory):
        self.thresholds = {
            "normal": 7.0,
            "high_vix": 7.5,
            "spy_stress": 8.0,
        }
        self.regime_stats = {
            "normal": {"trades": 0, "wins": 0},
            "high_vix": {"trades": 0, "wins": 0},
            "spy_stress": {"trades": 0, "wins": 0},
        }
    
    def adapt_threshold(self, regime: str):
        """Adjust threshold for specific regime."""
        stats = self.regime_stats[regime]
        if stats["trades"] < 10:
            return  # Need minimum sample
        
        win_rate = stats["wins"] / stats["trades"]
        current = self.thresholds[regime]
        
        if win_rate < 0.65:  # Below target
            self.thresholds[regime] = min(10.0, current + 0.5)
        elif win_rate > 0.85:  # Above target
            self.thresholds[regime] = max(5.0, current - 0.5)
```

**Implementation Steps**:
1. Split `agentic_outcomes` tracking by regime
2. Calculate win rate for normal/high_vix/spy_stress separately
3. Adapt thresholds independently every 10 trades per regime
4. Store in `system_config` table

**Expected Impact**: Better performance in volatile markets, regime-optimized filtering

---

### 3. Sector-Specific Gem Score Offsets
**Current**: GemScorer treats all sectors equally  
**Enhancement**: Learn which sectors perform better, adjust scores

```python
class SectorLearning:
    """Sector-specific performance tracking."""
    
    def __init__(self, memory: AgenticMemory):
        self.sector_offsets = {
            "TECH": 0.0,
            "HEALTHCARE": 0.0,
            "FINANCE": 0.0,
            "ENERGY": 0.0,
            "UNKNOWN": -0.5,  # Penalize unknown sectors
        }
    
    def adjust_gem_score(self, base_score: float, sector: str) -> float:
        """Apply sector offset to gem score."""
        offset = self.sector_offsets.get(sector, 0.0)
        return max(0.0, min(10.0, base_score + offset))
    
    def update_offsets(self):
        """Recalculate offsets based on sector win rates."""
        with sqlite3.connect(self.memory.agentic_db_path) as conn:
            sector_stats = conn.execute("""
                SELECT 
                    s.sector,
                    COUNT(*) as trades,
                    SUM(CASE WHEN o.return_pct > 0 THEN 1 ELSE 0 END) as wins
                FROM agentic_signals s
                JOIN agentic_fills f ON s.signal_id = f.signal_id
                JOIN agentic_outcomes o ON f.fill_id = o.fill_id
                GROUP BY s.sector
                HAVING COUNT(*) >= 5
            """).fetchall()
        
        baseline_wr = 0.70  # Target win rate
        for sector, trades, wins in sector_stats:
            sector_wr = wins / trades
            # If sector has 80% WR, add +0.5 to gem scores
            # If sector has 60% WR, subtract -0.5
            offset = (sector_wr - baseline_wr) * 5.0
            self.sector_offsets[sector] = round(offset, 1)
```

**Implementation Steps**:
1. Add sector tracking to all signals (already in schema)
2. Calculate sector-specific win rates after 5+ trades per sector
3. Apply offsets in Forecaster agent during confidence scoring
4. Update offsets every 20 trades

**Expected Impact**: +3-5% WR, better sector selection

---

## ðŸ“Š **Phase 2: Machine Learning (30-90 Days)**

### 4. Gradient Boosting for Confidence Scoring
Replace rule-based GemScorer with ML model trained on historical outcomes.

**Features to Train On**:
- Gap percentage
- Volume spike ratio
- RSI, MACD, Bollinger Band position
- Price action patterns (higher lows, trend)
- Sector
- Market regime
- Time of day (premarket vs open)

**Target**: Win probability (0.0-1.0)

**Model**: XGBoost or LightGBM  
**Training Data**: 6 years of backtest results (32 signals â†’ 8 trades)

```python
import xgboost as xgb

class MLGemScorer:
    """Machine learning-based gem score prediction."""
    
    def __init__(self):
        self.model = xgb.XGBClassifier(
            objective='binary:logistic',
            max_depth=6,
            n_estimators=100,
        )
        self.is_trained = False
    
    def train(self, X: pd.DataFrame, y: pd.Series):
        """Train on historical win/loss data."""
        self.model.fit(X, y)
        self.is_trained = True
    
    def predict_win_probability(self, features: Dict[str, float]) -> float:
        """Return 0.0-1.0 win probability."""
        X = pd.DataFrame([features])
        return self.model.predict_proba(X)[0][1]
    
    def score(self, signal_data: Dict) -> float:
        """Convert probability to 0-10 gem score."""
        prob = self.predict_win_probability(signal_data)
        return prob * 10.0
```

**Implementation Steps**:
1. Collect 100+ historical signals with outcomes (need more data)
2. Extract features from each signal
3. Train XGBoost model (win/loss classification)
4. Replace rule-based GemScorer with ML predictions
5. Retrain every 50 new trades

**Expected Impact**: +10-15% WR, better signal discrimination

---

### 5. Reinforcement Learning for Position Sizing
Current position sizing is fixed (1% per trade). RL agent can learn optimal sizing.

**State Space**:
- Confidence score
- Market regime
- Portfolio allocation
- Recent win streak/loss streak

**Action Space**:
- Position size: 0.5%, 1.0%, 1.5%, 2.0%

**Reward Function**:
- +1.0 for win * position_size
- -1.0 for loss * position_size
- Penalize overconcentration

**Algorithm**: Proximal Policy Optimization (PPO)

```python
import stable_baselines3 as sb3

class RLPositionSizer:
    """RL agent for dynamic position sizing."""
    
    def __init__(self):
        self.model = sb3.PPO('MlpPolicy', env=TradingEnv())
    
    def train(self, historical_data: List[Dict]):
        """Train on historical trade outcomes."""
        self.model.learn(total_timesteps=10000)
    
    def get_position_size(self, state: Dict) -> float:
        """Return optimal position size (0.005-0.02)."""
        action, _ = self.model.predict(state)
        return action * 0.01  # Scale to 0.5-2.0%
```

**Implementation Steps**:
1. Define trading environment (OpenAI Gym)
2. Collect 100+ historical trades
3. Train PPO agent on replay buffer
4. Replace fixed sizing in Trader agent
5. Retrain every 100 trades

**Expected Impact**: +20-30% total returns (better sizing = compounding)

---

## ðŸ“Š **Phase 3: Advanced Learning (90+ Days)**

### 6. LSTM for Market Regime Prediction
Current regime detection is reactive (VIX/SPY today). LSTM can predict regime changes.

**Features**: 30-day time series of VIX, SPY, volume, volatility  
**Target**: Regime 3 days ahead  
**Impact**: Pre-adjust thresholds before regime shifts

### 7. Attention Networks for News Sentiment
Replace keyword-based NewsSentry with transformer model (BERT/FinBERT).

**Impact**: More nuanced sentiment analysis, fewer false positives/negatives

### 8. Multi-Agent RL Coordination
Train agents to coordinate via shared reward function (maximize portfolio returns).

**Impact**: Agents learn to specialize (Forecaster â†’ high-confidence, RiskOfficer â†’ risk reduction)

---

## ðŸŽ¯ **Immediate Actionable Training (Today)**

We can implement **Phase 1 enhancements** right now:

### Option A: Agent Performance Weighting (2 hours)
1. Add tracking for agent accuracy to `agent_performance` table
2. Update Auditor to calculate weights after each trade
3. Replace binary veto with weighted consensus

### Option B: Regime-Specific Thresholds (1 hour)
1. Split threshold tracking by regime in `system_config`
2. Adapt thresholds independently per regime
3. Run another 6-year backtest to validate

### Option C: Sector Learning (1.5 hours)
1. Calculate sector win rates from historical data
2. Apply offsets in GemScorer
3. Validate on backtest data

---

## ðŸ“Š **Training Data Requirements**

| Enhancement | Min Trades Needed | Current Data | Status |
|-------------|-------------------|--------------|--------|
| Threshold Adaptation | 20 | 8 (backtest) | Need 12 more |
| Agent Weighting | 30 | 8 | Need 22 more |
| Sector Learning | 50 (5 per sector) | 8 | Need 42 more |
| ML GemScorer | 100+ | 32 (signals) | âœ… Enough signals |
| RL Position Sizing | 100+ | 8 | Need 92 more |

**Recommendation**: Start **30-day paper trading** to collect live data, then train ML models.

---

## ðŸš¦ **Decision Matrix**

| If Win Rate After 30 Days | Action |
|----------------------------|--------|
| **> 70%** | Deploy live with 0.5% risk, continue paper trading for ML data |
| **60-70%** | Implement Phase 1 enhancements, extend paper trading |
| **< 60%** | Debug agents, retrain thresholds, add more filters |

---

## ðŸŽ¯ **Recommended Next Steps**

1. **Begin 30-day paper trading** (collect real-world data)
2. **Implement Agent Performance Weighting** (2 hours, quick win)
3. **Implement Regime-Specific Thresholds** (1 hour, quick win)
4. **After 30 days + 30 trades**: Train XGBoost GemScorer (Phase 2)
5. **After 90 days + 100 trades**: Train RL Position Sizer (Phase 2)

---

## ðŸ“ **Training Checklist**

- [ ] Agent weighting system implemented
- [ ] Regime-specific thresholds active
- [ ] Sector learning offsets applied
- [ ] 30 trades collected from paper trading
- [ ] ML GemScorer trained and backtested
- [ ] RL position sizer trained
- [ ] LSTM regime predictor implemented
- [ ] Transformer news sentiment deployed

**Current Progress**: 3/8 training enhancements completed (37.5%)

---

**Next Action**: Choose Phase 1 enhancement to implement (Agent Weighting recommended for highest immediate impact).
