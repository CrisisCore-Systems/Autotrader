"""End-to-end workflow tests for complete token scanning."""

from __future__ import annotations

import pytest
from importlib.util import find_spec
from unittest.mock import Mock, patch, MagicMock
from datetime import datetime

try:
    from src.core.pipeline import HiddenGemScanner, TokenConfig, ScanContext
    from src.core.tree import TreeNode, NodeOutcome
except ModuleNotFoundError as exc:  # pragma: no cover - skip when optional deps missing
    pytest.skip(
        f"Token scanning workflow tests require optional dependency: {exc}",
        allow_module_level=True,
    )


# ============================================================================
# Complete Scanning Workflow Tests
# ============================================================================

REQUESTS_AVAILABLE = find_spec("requests") is not None


class TestTokenScanningWorkflow:
    """Test complete token scanning workflow from start to finish."""
    
    @pytest.fixture
    def token_config(self):
        """Create sample token configuration."""
        return TokenConfig(
            symbol="TEST",
            coingecko_id="test-token",
            defillama_slug="test",
            contract_address="0x123456789",
            narratives=["DeFi", "Layer2"],
            glyph="â§—"
        )
    
    @pytest.fixture
    def scanner(self):
        """Create scanner instance."""
        from unittest.mock import Mock
        mock_coin_client = Mock()
        return HiddenGemScanner(coin_client=mock_coin_client)
    
    @pytest.mark.skipif(not REQUESTS_AVAILABLE, reason="requests dependency is required for NewsClient")
    def test_end_to_end_scan_workflow(self, scanner, token_config):
        """Test complete scanning workflow with all components."""
        # Create scan context
        context = ScanContext(config=token_config)
        
        # Verify context setup
        assert context.config.symbol == "TEST"
        assert len(context.config.narratives) == 2
        
        # Mock external API calls
        with patch('src.core.clients.CoinGeckoClient.fetch_market_chart') as mock_cg, \
             patch('src.core.clients.EtherscanClient.fetch_contract_source') as mock_eth, \
             patch('src.core.news_client.NewsClient.get_news_for_token') as mock_news:
            
            # Setup mocks
            mock_cg.return_value = {
                "price": 1.5,
                "market_cap": 1000000,
                "volume_24h": 500000,
                "price_change_24h": 5.2
            }
            
            mock_eth.return_value = {
                "ContractName": "TestToken",
                "SourceCode": "contract TestToken {}",
                "verified": True
            }
            
            mock_news.return_value = [
                {
                    "title": "Test Token Launches",
                    "summary": "New DeFi protocol launches",
                    "source": "CoinDesk",
                    "sentiment": 7
                }
            ]
            
            # Run scanner (may not complete fully in test environment)
            try:
                result = scanner.scan(token_config)
                
                # If scan completes, verify result structure
                if result:
                    assert "symbol" in result
                    assert "final_score" in result
                    assert "gem_score" in result
            except Exception as e:
                # Expected in test environment without full API keys
                pytest.skip(f"Scan requires full environment: {e}")
    
    def test_execution_tree_construction(self, token_config):
        """Test that execution tree is properly constructed."""
        from src.core.tree import TreeNode
        
        # Create root node
        root = TreeNode(
            key="root",
            title="Scan Test Token",
            description="Complete token analysis",
            action=lambda ctx: NodeOutcome(status="success", summary="", data={}, proceed=True)
        )
        
        # Add child nodes
        market_node = TreeNode(
            key="market_data",
            title="Fetch Market Data",
            description="Get price and volume data",
            action=lambda ctx: NodeOutcome(status="success", summary="Fetched price", data={"price": 1.5}, proceed=True)
        )
        
        root.add_child(market_node)
        
        # Verify tree structure
        assert root.key == "root"
        assert len(root.children) == 1
        assert root.children[0].key == "market_data"
    
    def test_feature_extraction_pipeline(self, token_config):
        """Test feature extraction from raw data."""
        raw_data = {
            "price": 1.5,
            "volume_24h": 1000000,
            "market_cap": 5000000,
            "holders": 10000,
            "liquidity_usd": 500000
        }
        
        # Extract features
        features = {}
        features["price_log"] = __import__('math').log(raw_data["price"] + 1)
        features["volume_log"] = __import__('math').log(raw_data["volume_24h"] + 1)
        features["holder_count"] = raw_data["holders"]
        features["liquidity_ratio"] = raw_data["liquidity_usd"] / raw_data["market_cap"]
        
        # Verify features
        assert all(isinstance(v, (int, float)) for v in features.values())
        assert 0 <= features["liquidity_ratio"] <= 1
    
    def test_scoring_pipeline(self):
        """Test gem score calculation pipeline."""
        # Sample feature values
        features = {
            "liquidity_score": 7.5,
            "holder_growth": 8.0,
            "volume_score": 6.5,
            "sentiment_score": 7.0,
            "security_score": 9.0,
            "narrative_momentum": 8.5,
            "technical_score": 7.5,
            "social_buzz": 6.0
        }
        
        # Calculate weighted score
        weights = {
            "liquidity_score": 0.15,
            "holder_growth": 0.12,
            "volume_score": 0.10,
            "sentiment_score": 0.15,
            "security_score": 0.20,
            "narrative_momentum": 0.10,
            "technical_score": 0.10,
            "social_buzz": 0.08
        }
        
        gem_score = sum(features[k] * weights[k] for k in features.keys())
        
        assert 0 <= gem_score <= 10
    
    def test_safety_analysis_pipeline(self):
        """Test safety analysis in pipeline."""
        from src.core.safety import SafetyAnalyzer
        
        analyzer = SafetyAnalyzer()
        
        contract_data = {
            "SourceCode": "pragma solidity ^0.8.0; contract Safe {}",
            "ContractName": "SafeToken",
            "CompilerVersion": "v0.8.0"
        }
        
        report = analyzer.analyze_contract(contract_data)
        
        assert "score" in report
        assert "severity" in report
        assert "findings" in report
        assert report["severity"] in ["low", "medium", "high", "critical", "none"]
    
    def test_sentiment_aggregation_pipeline(self):
        """Test sentiment aggregation from multiple sources."""
        news_sentiment = 0.7
        twitter_sentiment = 0.6
        reddit_sentiment = 0.65
        
        # Weighted average
        weights = {"news": 0.4, "twitter": 0.4, "reddit": 0.2}
        
        aggregate_sentiment = (
            news_sentiment * weights["news"] +
            twitter_sentiment * weights["twitter"] +
            reddit_sentiment * weights["reddit"]
        )
        
        assert 0 <= aggregate_sentiment <= 1
        assert abs(aggregate_sentiment - 0.65) < 0.01
    
    def test_narrative_analysis_integration(self):
        """Test narrative analysis integration."""
        from src.core.narrative import NarrativeAnalyzer
        
        analyzer = NarrativeAnalyzer(use_llm=False)  # Use fallback
        
        narratives = [
            "DeFi protocol launches new staking feature",
            "Partnership with major exchange announced",
            "Community growth exceeds expectations"
        ]
        
        insight = analyzer.analyze(narratives)
        
        assert 0 <= insight.sentiment_score <= 1
        assert 0 <= insight.momentum <= 1
        assert isinstance(insight.themes, list)
    
    def test_artifact_generation(self, token_config):
        """Test artifact generation from scan results."""
        scan_results = {
            "symbol": "TEST",
            "final_score": 7.5,
            "gem_score": 7.8,
            "confidence": 0.85,
            "safety_report": {
                "score": 8.5,
                "severity": "low",
                "findings": []
            },
            "narrative": {
                "sentiment_score": 0.7,
                "themes": ["DeFi", "Innovation"]
            }
        }
        
        # Generate artifact markdown
        markdown = f"""# {scan_results['symbol']} Analysis Report
        
## Summary
- **Final Score**: {scan_results['final_score']}/10
- **GemScore**: {scan_results['gem_score']}/10
- **Confidence**: {scan_results['confidence']*100:.1f}%

## Safety Analysis
- **Score**: {scan_results['safety_report']['score']}/10
- **Severity**: {scan_results['safety_report']['severity']}

## Narrative Analysis
- **Sentiment**: {scan_results['narrative']['sentiment_score']*100:.1f}%
- **Themes**: {', '.join(scan_results['narrative']['themes'])}
"""
        
        assert scan_results['symbol'] in markdown
        assert "Final Score" in markdown
        assert "Safety Analysis" in markdown


# ============================================================================
# Multi-Token Batch Processing Tests
# ============================================================================

class TestBatchProcessing:
    """Test batch processing of multiple tokens."""
    
    def test_batch_scan_multiple_tokens(self):
        """Test scanning multiple tokens in batch."""
        tokens = [
            TokenConfig(symbol="BTC", coingecko_id="bitcoin", defillama_slug="bitcoin", contract_address="0xBTC"),
            TokenConfig(symbol="ETH", coingecko_id="ethereum", defillama_slug="ethereum", contract_address="0xETH"),
            TokenConfig(symbol="SOL", coingecko_id="solana", defillama_slug="solana", contract_address="0xSOL")
        ]
        
        results = []
        for token in tokens:
            # Mock scan result
            result = {
                "symbol": token.symbol,
                "final_score": 7.0,
                "gem_score": 7.5,
                "confidence": 0.8
            }
            results.append(result)
        
        assert len(results) == 3
        assert all(r["symbol"] in ["BTC", "ETH", "SOL"] for r in results)
    
    def test_parallel_feature_extraction(self):
        """Test parallel feature extraction for multiple tokens."""
        import concurrent.futures
        
        def extract_features(symbol):
            return {
                "symbol": symbol,
                "features": {"price_log": 1.5, "volume_log": 14.0}
            }
        
        symbols = ["BTC", "ETH", "SOL", "ADA", "DOT"]
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            results = list(executor.map(extract_features, symbols))
        
        assert len(results) == 5
        assert all("features" in r for r in results)


# ============================================================================
# Data Pipeline Integration Tests
# ============================================================================

class TestDataPipelineIntegration:
    """Test data flow through the entire pipeline."""
    
    def test_data_ingestion_to_storage(self):
        """Test data flows from ingestion to feature store."""
        from src.core.feature_store import FeatureStore, FeatureMetadata, FeatureType, FeatureCategory
        
        store = FeatureStore()
        
        # Register the price feature in schema
        price_metadata = FeatureMetadata(
            name="price",
            feature_type=FeatureType.NUMERIC,
            category=FeatureCategory.MARKET,
            description="Token price in USD",
            unit="USD",
            min_value=0.0,
            nullable=False
        )
        store.register_feature(price_metadata)
        
        # Simulate data ingestion
        raw_data = {
            "token": "TEST",
            "price": 1.5,
            "volume": 1000000,
            "holders": 5000
        }
        
        # Transform and store
        store.write_feature(
            feature_name="price",
            value=raw_data["price"],
            token_symbol=raw_data["token"],
            confidence=0.9,
        )
        
        # Retrieve and verify
        feature_value = store.read_feature("price", "TEST")
        assert feature_value is not None
        assert feature_value.value == raw_data["price"]
    
    def test_feature_engineering_transforms(self):
        """Test feature engineering transforms are applied."""
        import math
        
        raw_value = 1000000
        
        # Apply transforms
        log_transform = math.log(raw_value + 1)
        sqrt_transform = math.sqrt(raw_value)
        normalized = (raw_value - 0) / (10000000 - 0)  # Min-max normalize
        
        assert log_transform > 0
        assert sqrt_transform > 0
        assert 0 <= normalized <= 1
    
    def test_multi_source_data_aggregation(self):
        """Test aggregating data from multiple sources."""
        # Mock data from different sources
        coingecko_data = {"price": 1.50, "market_cap": 1000000}
        etherscan_data = {"holders": 5000, "verified": True}
        defillama_data = {"tvl": 500000, "liquidity": 250000}
        
        # Aggregate
        aggregated = {
            **coingecko_data,
            **etherscan_data,
            **defillama_data
        }
        
        assert "price" in aggregated
        assert "holders" in aggregated
        assert "tvl" in aggregated
        assert len(aggregated) == 6


# ============================================================================
# Error Recovery and Resilience Tests
# ============================================================================

class TestErrorRecoveryWorkflow:
    """Test error recovery in scanning workflow."""
    
    def test_partial_scan_completion(self):
        """Test that partial scans can complete with available data."""
        results = {
            "market_data": {"price": 1.5},  # Success
            "contract_data": None,  # Failed
            "news_data": [{"title": "News"}],  # Success
            "sentiment_data": None  # Failed
        }
        
        # Should still be able to calculate score with partial data
        available_data_count = sum(1 for v in results.values() if v is not None)
        confidence = available_data_count / len(results)
        
        assert 0 < confidence < 1
        assert confidence == 0.5  # 2 out of 4 succeeded
    
    def test_fallback_to_cached_data(self):
        """Test falling back to cached data on API failure."""
        from src.services.cache_policy import CachePolicy
        
        cache = CachePolicy(name="test", ttl_seconds=300, max_size=100)
        
        # Store cached data
        cache.set("TEST:price", 1.5)
        
        # Simulate API failure
        api_result = None
        
        # Fallback to cache
        result = api_result or cache.get("TEST:price")
        
        assert result == 1.5
    
    def test_retry_logic_for_failed_requests(self):
        """Test retry logic for failed API requests."""
        max_retries = 3
        attempts = 0
        
        def unreliable_api_call():
            nonlocal attempts
            attempts += 1
            if attempts < 3:
                raise Exception("API Error")
            return {"success": True}
        
        # Retry logic
        for i in range(max_retries):
            try:
                result = unreliable_api_call()
                break
            except Exception:
                if i == max_retries - 1:
                    result = None
                continue
        
        assert result is not None
        assert attempts == 3
